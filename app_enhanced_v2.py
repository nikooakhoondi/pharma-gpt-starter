# app_enhanced_v2.py â€” Supabase-only: Pivot + Filter/Table + GPT Chat
from typing import Optional
import os, json
import pandas as pd
import streamlit as st
import streamlit_authenticator as stauth
from supabase import create_client, Client
from openai import OpenAI

# ---------------------------- Page config ----------------------------
st.set_page_config(page_title="Pharma-GPT (Supabase)", layout="wide")

# ---------------------------- Auth ----------------------------
def do_login(authenticator):
    try:
        return authenticator.login(location="main")
    except TypeError:
        pass
    try:
        return authenticator.login("Login", "main")
    except TypeError:
        return authenticator.login("Login", location="main")

def _build_authenticator_from_secrets():
    AUTH = st.secrets.get("auth", {})
    cookie_name = str(AUTH.get("cookie_name", "pharma_gpt_auth"))
    cookie_key  = str(AUTH.get("cookie_key",  "change-me"))
    cookie_days = int(AUTH.get("cookie_expiry_days", 7))

    users = {}
    creds_in = AUTH.get("credentials", {}).get("usernames", {})
    for uname, info in creds_in.items():
        users[str(uname)] = {
            "name":     str(info.get("name", "")),
            "email":    str(info.get("email", "")),
            "password": str(info.get("password", "")),
        }

    if not users:
        st.error("No users found in secrets. Add them under [auth] â†’ [auth.credentials.usernames.*] in `.streamlit/secrets.toml`.")
        st.stop()

    return stauth.Authenticate({"usernames": users}, cookie_name, cookie_key, cookie_days)

authenticator = _build_authenticator_from_secrets()
name, auth_status, username = do_login(authenticator)
if not auth_status:
    if auth_status is False:
        st.error("Incorrect username or password")
    else:
        st.info("Please log in to continue.")
    st.stop()
else:
    try:
        authenticator.logout("Logout", location="sidebar")
    except TypeError:
        authenticator.logout("Logout", "sidebar")

# ---------------------------- Supabase ----------------------------
@st.cache_resource
def get_supabase() -> Optional[Client]:
    url = st.secrets.get("SUPABASE_URL")
    key = st.secrets.get("SUPABASE_KEY")
    if not url or not key:
        st.error("Missing SUPABASE_URL / SUPABASE_KEY in Streamlit Secrets.")
        return None
    return create_client(url, key)

sb = get_supabase()
if sb is None:
    st.stop()

TABLE = "Amarname_sheet1"  # â† keep exactly as you asked

# ---------------------------- Preface ----------------------------
st.title("ğŸ’Š Pharma-GPT")
st.caption("Pivot like Excel â€” or ask in natural language. Results come from your Supabase table: Amarname_sheet1.")

with st.expander("Ø±Ø§Ù‡Ù†Ù…Ø§ÛŒ Ø³Ø±ÛŒØ¹ / Quick Start", expanded=True):
    st.markdown(
        """
**Ø¯Ùˆ Ù…Ø³ÛŒØ± Ø¯Ø§Ø±ÛŒØ¯:**
1) **Pivot**: Ø¯Ùˆ Ø¨ÙØ¹Ø¯ + ÛŒÚ© Ù…ØªØ±ÛŒÚ© Ø±Ø§ Ø§Ù†ØªØ®Ø§Ø¨ Ú©Ù†ÛŒØ¯ ØªØ§ Ø¬Ù…Ø¹â€ŒÙ‡Ø§ Ø±Ø§ Ø±ÙˆÛŒ Ú©Ù„ Ø¯ÛŒØªØ§Ø¨ÛŒØ³ Ø¨Ø¨ÛŒÙ†ÛŒØ¯.  
2) **ÙÛŒÙ„ØªØ±/Ø¬Ø¯ÙˆÙ„**: Ø¨Ø§ Ø¬Ø¹Ø¨Ù‡â€ŒÙ‡Ø§ÛŒ Ø¬Ø³ØªØ¬ÙˆØŒ Ø¯Ø§Ø¯Ù‡ Ø±Ø§ Ø¨Ø± Ø§Ø³Ø§Ø³ **Ù…ÙˆÙ„Ú©ÙˆÙ„ØŒ Ø¨Ø±Ù†Ø¯ØŒ Ø´Ú©Ù„ Ø¯Ø§Ø±ÙˆÛŒÛŒØŒ Ù…Ø³ÛŒØ± Ù…ØµØ±ÙØŒ ØªØ§Ù…ÛŒÙ†â€ŒÚ©Ù†Ù†Ø¯Ù‡ØŒ Ø³Ø§Ù„ØŒ ATC ÛŒØ§ ØªÙˆÙ„ÛŒØ¯ÛŒ/ÙˆØ§Ø±Ø¯Ø§ØªÛŒ** ÙÛŒÙ„ØªØ± Ùˆ Ù…Ø±ØªØ¨ Ú©Ù†ÛŒØ¯ Ùˆ CSV Ø¨Ú¯ÛŒØ±ÛŒØ¯.  

**Chat**: Ø¨Ù‡ ÙØ§Ø±Ø³ÛŒ/Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒ Ø¨Ù¾Ø±Ø³ÛŒØ¯ (Ù…Ø«Ù„Ø§Ù‹: Â«Ø³Ù‡Ù… Ø§Ø±Ø²Ø´ Ø±ÛŒØ§Ù„ÛŒ Ù‡Ø± Ø´Ø±Ú©Øª Ø¯Ø± Û±Û´Û°Û°â€“Û±Û´Û°Û²ØŸÂ») ØªØ§ Ø®Ø±ÙˆØ¬ÛŒ Ø¬Ø¯ÙˆÙ„/Ú†Ø§Ø±Øª Ø¨Ú¯ÛŒØ±ÛŒØ¯.  
**Ù†Ú©ØªÙ‡ ATC**: Ù‡Ù… Ø§Ù†ØªØ®Ø§Ø¨ Ø¯Ù‚ÛŒÙ‚ Ø¯Ø§Ø±ÛŒØ¯ØŒ Ù‡Ù… Ù¾ÛŒØ´ÙˆÙ†Ø¯ (Ù…Ø«Ù„ `N06A%`).  
"""
    )

# ---------------------------- Tabs ----------------------------
tab_pivot, tab_table, tab_chat = st.tabs(["ğŸ“Š Pivot", "ğŸ“‹ Filter/Table", "ğŸ’¬ Chat"])

# ============================ PIVOT ============================
with tab_pivot:
    allowed_dims = [
        "Ø³Ø§Ù„","Ú©Ø¯ Ú˜Ù†Ø±ÛŒÚ©","Ù†Ø§Ù… Ú˜Ù†Ø±ÛŒÚ©","Ù…ÙˆÙ„Ú©ÙˆÙ„ Ø¯Ø§Ø±ÙˆÛŒÛŒ","Ù†Ø§Ù… ØªØ¬Ø§Ø±ÛŒ ÙØ±Ø¢ÙˆØ±Ø¯Ù‡",
        "Ø´Ø±Ú©Øª ØªØ§Ù…ÛŒÙ† Ú©Ù†Ù†Ø¯Ù‡","ØªÙˆÙ„ÛŒØ¯ÛŒ/ÙˆØ§Ø±Ø¯Ø§ØªÛŒ","route","dosage form","atc code","Anatomical",
    ]
    metrics = ["Ø§Ø±Ø²Ø´ Ø±ÛŒØ§Ù„ÛŒ", "Ù‚ÛŒÙ…Øª", "ØªØ¹Ø¯Ø§Ø¯ ØªØ§Ù…ÛŒÙ† Ø´Ø¯Ù‡"]

    c1, c2, c3 = st.columns(3)
    dim1   = c1.selectbox("Dimension 1", allowed_dims, index=0)
    dim2   = c2.selectbox("Dimension 2", allowed_dims, index=5)
    metric = c3.selectbox("Metric (sum of)", metrics, index=0)
    y1, y2 = st.slider("Year range (Ø³Ø§Ù„)", min_value=1390, max_value=1500, value=(1400, 1404))

   @st.cache_data(ttl=300)
def query_with_filters(
    mols, brands, forms, routes, provs, years, atc_exact, atc_prefix, prod_type,
    sort_by, descending, limit_rows
):
    q = sb.table(TABLE).select("*")

    if mols:      q = q.in_(COLS["Ù…ÙˆÙ„Ú©ÙˆÙ„ Ø¯Ø§Ø±ÙˆÛŒÛŒ"], mols)
    if brands:    q = q.in_(COLS["Ù†Ø§Ù… Ø¨Ø±Ù†Ø¯"], brands)
    if forms:     q = q.in_(COLS["Ø´Ú©Ù„ Ø¯Ø§Ø±ÙˆÛŒÛŒ"], forms)
    if routes:    q = q.in_(COLS["Ø·Ø±ÛŒÙ‚Ù‡ Ù…ØµØ±Ù"], routes)
    if provs:     q = q.in_(COLS["Ù†Ø§Ù… ØªØ§Ù…ÛŒÙ† Ú©Ù†Ù†Ø¯Ù‡"], provs)
    if years:     q = q.in_(COLS["Ø³Ø§Ù„"], years)
    if prod_type: q = q.in_(COLS["ÙˆØ§Ø±Ø¯Ø§ØªÛŒ/ØªÙˆÙ„ÛŒØ¯ Ø¯Ø§Ø®Ù„"], prod_type)

    # ATC: exact first; else prefix
    if atc_exact:
        q = q.in_(COLS["ATC code"], atc_exact)
    elif atc_prefix.strip():
        try:
            q = q.ilike(COLS["ATC code"], atc_prefix.strip() + "%")
        except Exception:
            q = q.like(COLS["ATC code"], atc_prefix.strip() + "%")

    # âš ï¸ Avoid server-side order() because of non-ASCII / spaced column names â†’ do it client-side
    try:
        res = q.limit(int(limit_rows)).execute()
    except Exception as e:
        # last-resort fallback (no filters changed)
        st.error(f"Supabase query failed: {e}")
        return pd.DataFrame()

    df = pd.DataFrame(res.data or [])

    # Client-side sort (safe for any column name)
    if not df.empty and sort_by in df.columns:
        df = df.sort_values(sort_by, ascending=not descending, kind="mergesort")

    return df

# ============================ FILTER / TABLE ============================
with tab_table:
    COLS = {
        "Ù…ÙˆÙ„Ú©ÙˆÙ„ Ø¯Ø§Ø±ÙˆÛŒÛŒ": "Ù…ÙˆÙ„Ú©ÙˆÙ„ Ø¯Ø§Ø±ÙˆÛŒÛŒ",
        "Ù†Ø§Ù… Ø¨Ø±Ù†Ø¯": "Ù†Ø§Ù… ØªØ¬Ø§Ø±ÛŒ ÙØ±Ø¢ÙˆØ±Ø¯Ù‡",
        "Ø´Ú©Ù„ Ø¯Ø§Ø±ÙˆÛŒÛŒ": "dosage form",
        "Ø·Ø±ÛŒÙ‚Ù‡ Ù…ØµØ±Ù": "route",
        "Ù†Ø§Ù… ØªØ§Ù…ÛŒÙ† Ú©Ù†Ù†Ø¯Ù‡": "Ø´Ø±Ú©Øª ØªØ§Ù…ÛŒÙ† Ú©Ù†Ù†Ø¯Ù‡",
        "Ø³Ø§Ù„": "Ø³Ø§Ù„",
        "ATC code": "atc code",
        "ÙˆØ§Ø±Ø¯Ø§ØªÛŒ/ØªÙˆÙ„ÛŒØ¯ Ø¯Ø§Ø®Ù„": "ØªÙˆÙ„ÛŒØ¯ÛŒ/ÙˆØ§Ø±Ø¯Ø§ØªÛŒ",
        # for sorting
        "Ø§Ø±Ø²Ø´ Ø±ÛŒØ§Ù„ÛŒ": "Ø§Ø±Ø²Ø´ Ø±ÛŒØ§Ù„ÛŒ",
        "ØªØ¹Ø¯Ø§Ø¯ ØªØ§Ù…ÛŒÙ† Ø´Ø¯Ù‡": "ØªØ¹Ø¯Ø§Ø¯ ØªØ§Ù…ÛŒÙ† Ø´Ø¯Ù‡",
        "Ù‚ÛŒÙ…Øª": "Ù‚ÛŒÙ…Øª",
    }

    @st.cache_data(ttl=600)
    def get_unique(col: str, limit: int = 20000):
        """
        Ultra-robust distinct fetch for problematic column names (spaces, slashes, non-ASCII).
        Strategy: pull a small page with select("*") then dedupe client-side.
        """
        try:
            r = sb.table(TABLE).select("*").limit(limit).execute()
        except Exception as e:
            st.error(f"Supabase select failed for uniques on '{col}': {e}")
            return []

        data = r.data or []
        vals = []
        for row in data:
            v = row.get(col)
            if v is None:
                continue
            if isinstance(v, str) and v.strip() == "":
                continue
            vals.append(v)

        try:
            return sorted(set(vals))
        except TypeError:
            return sorted({str(v) for v in vals})

    st.subheader("ÙÛŒÙ„ØªØ±Ù‡Ø§")
    c1, c2 = st.columns(2)
    with c1:
        mols   = st.multiselect("Ù…ÙˆÙ„Ú©ÙˆÙ„ Ø¯Ø§Ø±ÙˆÛŒÛŒ", options=get_unique(COLS["Ù…ÙˆÙ„Ú©ÙˆÙ„ Ø¯Ø§Ø±ÙˆÛŒÛŒ"]))
        brands = st.multiselect("Ù†Ø§Ù… Ø¨Ø±Ù†Ø¯", options=get_unique(COLS["Ù†Ø§Ù… Ø¨Ø±Ù†Ø¯"]))
        forms  = st.multiselect("Ø´Ú©Ù„ Ø¯Ø§Ø±ÙˆÛŒÛŒ", options=get_unique(COLS["Ø´Ú©Ù„ Ø¯Ø§Ø±ÙˆÛŒÛŒ"]))
        routes = st.multiselect("Ø·Ø±ÛŒÙ‚Ù‡ Ù…ØµØ±Ù", options=get_unique(COLS["Ø·Ø±ÛŒÙ‚Ù‡ Ù…ØµØ±Ù"]))
    with c2:
        provs  = st.multiselect("Ù†Ø§Ù… ØªØ§Ù…ÛŒÙ† Ú©Ù†Ù†Ø¯Ù‡", options=get_unique(COLS["Ù†Ø§Ù… ØªØ§Ù…ÛŒÙ† Ú©Ù†Ù†Ø¯Ù‡"]))
        years  = st.multiselect("Ø³Ø§Ù„", options=get_unique(COLS["Ø³Ø§Ù„"]))
        atc_exact = st.multiselect("ATC code (Exact)", options=get_unique(COLS["ATC code"]))
        atc_prefix = st.text_input("ÙÛŒÙ„ØªØ± ATC Ø¨Ø± Ø§Ø³Ø§Ø³ Ù¾ÛŒØ´ÙˆÙ†Ø¯ (Ù…Ø«Ù„ N06A)", value="")

    prod_type = st.multiselect("ÙˆØ§Ø±Ø¯Ø§ØªÛŒ/ØªÙˆÙ„ÛŒØ¯ Ø¯Ø§Ø®Ù„", options=get_unique(COLS["ÙˆØ§Ø±Ø¯Ø§ØªÛŒ/ØªÙˆÙ„ÛŒØ¯ Ø¯Ø§Ø®Ù„"]))

    st.markdown("---")
    colA, colB, colC = st.columns(3)
    with colA:
        sort_by = st.selectbox(
            "Ù…Ø±ØªØ¨â€ŒØ³Ø§Ø²ÛŒ Ø¨Ø± Ø§Ø³Ø§Ø³",
            options=[COLS["Ø§Ø±Ø²Ø´ Ø±ÛŒØ§Ù„ÛŒ"], COLS["ØªØ¹Ø¯Ø§Ø¯ ØªØ§Ù…ÛŒÙ† Ø´Ø¯Ù‡"], COLS["Ù‚ÛŒÙ…Øª"], COLS["Ø³Ø§Ù„"]],
            format_func=lambda c: [k for k, v in COLS.items() if v == c][0]
        )
    with colB:
        descending = st.toggle("Ù†Ø²ÙˆÙ„ÛŒ", value=True)
    with colC:
        limit_rows = st.number_input("Ø­Ø¯Ø§Ú©Ø«Ø± Ø±Ø¯ÛŒÙ", value=20000, min_value=1000, step=1000)

    reset = st.button("Ø¨Ø§Ø²Ù†Ø´Ø§Ù†ÛŒ ÙÛŒÙ„ØªØ±Ù‡Ø§")
    if reset:
        st.experimental_rerun()

    @st.cache_data(ttl=300)
    def query_with_filters(
        mols, brands, forms, routes, provs, years, atc_exact, atc_prefix, prod_type, sort_by, descending, limit_rows
    ):
        q = sb.table(TABLE).select("*")
        if mols:      q = q.in_(COLS["Ù…ÙˆÙ„Ú©ÙˆÙ„ Ø¯Ø§Ø±ÙˆÛŒÛŒ"], mols)
        if brands:    q = q.in_(COLS["Ù†Ø§Ù… Ø¨Ø±Ù†Ø¯"], brands)
        if forms:     q = q.in_(COLS["Ø´Ú©Ù„ Ø¯Ø§Ø±ÙˆÛŒÛŒ"], forms)
        if routes:    q = q.in_(COLS["Ø·Ø±ÛŒÙ‚Ù‡ Ù…ØµØ±Ù"], routes)
        if provs:     q = q.in_(COLS["Ù†Ø§Ù… ØªØ§Ù…ÛŒÙ† Ú©Ù†Ù†Ø¯Ù‡"], provs)
        if years:     q = q.in_(COLS["Ø³Ø§Ù„"], years)
        if prod_type: q = q.in_(COLS["ÙˆØ§Ø±Ø¯Ø§ØªÛŒ/ØªÙˆÙ„ÛŒØ¯ Ø¯Ø§Ø®Ù„"], prod_type)

        # ATC: exact first; else prefix
        if atc_exact:
            q = q.in_(COLS["ATC code"], atc_exact)
        elif atc_prefix.strip():
            try:
                q = q.ilike(COLS["ATC code"], atc_prefix.strip() + "%")
            except Exception:
                q = q.like(COLS["ATC code"], atc_prefix.strip() + "%")

        q = q.order(sort_by, desc=descending).limit(int(limit_rows))
        res = q.execute()
        df = pd.DataFrame(res.data or [])

        # Client-side fallback sort
        if not df.empty and sort_by in df.columns:
            df = df.sort_values(sort_by, ascending=not descending)

        return df

    df = query_with_filters(mols, brands, forms, routes, provs, years, atc_exact, atc_prefix, prod_type, sort_by, descending, limit_rows)
    st.markdown("### Ø®Ø±ÙˆØ¬ÛŒ ÙÛŒÙ„ØªØ± Ø´Ø¯Ù‡")
    st.dataframe(df, use_container_width=True, hide_index=True)
    st.download_button("Ø¯Ø§Ù†Ù„ÙˆØ¯ CSV", df.to_csv(index=False).encode("utf-8-sig"), "filtered.csv", "text/csv")

# ============================ GPT DATA CHAT ============================
with tab_chat:
    API_KEY = st.secrets.get("OPENAI_API_KEY") or os.getenv("OPENAI_API_KEY")
    st.subheader("Ú¯ÙØªÚ¯Ùˆ Ø¨Ø§ Ø¯ÛŒØªØ§Ø¨ÛŒØ³")

    if not API_KEY:
        st.warning("OpenAI API key not found â€” data chat is disabled.")
    else:
        client = OpenAI(api_key=API_KEY)

        allowed_dims = [
            "Ø³Ø§Ù„","Ú©Ø¯ Ú˜Ù†Ø±ÛŒÚ©","Ù†Ø§Ù… Ú˜Ù†Ø±ÛŒÚ©","Ù…ÙˆÙ„Ú©ÙˆÙ„ Ø¯Ø§Ø±ÙˆÛŒÛŒ","Ù†Ø§Ù… ØªØ¬Ø§Ø±ÛŒ ÙØ±Ø¢ÙˆØ±Ø¯Ù‡",
            "Ø´Ø±Ú©Øª ØªØ§Ù…ÛŒÙ† Ú©Ù†Ù†Ø¯Ù‡","ØªÙˆÙ„ÛŒØ¯ÛŒ/ÙˆØ§Ø±Ø¯Ø§ØªÛŒ","route","dosage form","atc code","Anatomical",
        ]
        allowed_metrics = ["Ø§Ø±Ø²Ø´ Ø±ÛŒØ§Ù„ÛŒ","Ù‚ÛŒÙ…Øª","ØªØ¹Ø¯Ø§Ø¯ ØªØ§Ù…ÛŒÙ† Ø´Ø¯Ù‡"]
        allowed_filter_cols = allowed_dims[:]  # same list

        synonyms = {
            "Ø´Ø±Ú©Øª": "Ø´Ø±Ú©Øª ØªØ§Ù…ÛŒÙ† Ú©Ù†Ù†Ø¯Ù‡",
            "ØªØ§Ù…ÛŒÙ† Ú©Ù†Ù†Ø¯Ù‡": "Ø´Ø±Ú©Øª ØªØ§Ù…ÛŒÙ† Ú©Ù†Ù†Ø¯Ù‡",
            "Ú˜Ù†Ø±ÛŒÚ©": "Ù†Ø§Ù… Ú˜Ù†Ø±ÛŒÚ©",
            "Ù†Ø§Ù… ØªØ¬Ø§Ø±ÛŒ": "Ù†Ø§Ù… ØªØ¬Ø§Ø±ÛŒ ÙØ±Ø¢ÙˆØ±Ø¯Ù‡",
            "Ú©Ø¯": "Ú©Ø¯ Ú˜Ù†Ø±ÛŒÚ©",
            "Ø³Ø§Ù„ Ø´Ù…Ø³ÛŒ": "Ø³Ø§Ù„",
            "Ù…Ø³ÛŒØ±": "route",
            "Ø´Ú©Ù„": "dosage form",
            "Ø§Ø±Ø²Ø´": "Ø§Ø±Ø²Ø´ Ø±ÛŒØ§Ù„ÛŒ",
            "Ù‚ÛŒÙ…Øª ÙˆØ§Ø­Ø¯": "Ù‚ÛŒÙ…Øª",
            "ØªØ¹Ø¯Ø§Ø¯": "ØªØ¹Ø¯Ø§Ø¯ ØªØ§Ù…ÛŒÙ† Ø´Ø¯Ù‡",
            "ATC": "atc code",
        }

        guide = {"allowed_dims": allowed_dims, "allowed_metrics": allowed_metrics, "allowed_filters": allowed_filter_cols}

        if "data_chat" not in st.session_state:
            st.session_state.data_chat = []

        for msg in st.session_state.data_chat:
            st.chat_message(msg["role"]).write(msg["content"])

        user_q = st.chat_input("Ù…Ø«Ù„Ø§Ù‹: Ø³Ù‡Ù… Ø§Ø±Ø²Ø´ Ø±ÛŒØ§Ù„ÛŒ Ù‡Ø± Ø´Ø±Ú©Øª Ø¯Ø± Ø³Ø§Ù„â€ŒÙ‡Ø§ÛŒ Û±Û´Û°Û° ØªØ§ Û±Û´Û°Û²ØŸ")
        if user_q:
            st.chat_message("user").write(user_q)
            st.session_state.data_chat.append({"role": "user", "content": user_q})

            system_prompt = f"""
You are a planner that outputs ONLY compact JSON (no prose). You control a data tool with:
- pivot(dim1, dim2, metric, year_from, year_to)  # both dims must be from allowed_dims; metric from allowed_metrics
- rows(filters, limit)  # filters is an object of column -> value OR list of values; only columns in allowed_filters

Rules:
- Use Persian or English inputs.
- If the user asks for shares or totals by categories, use "pivot".
- If the user wants raw examples/records, use "rows".
- Respect year ranges if mentioned; otherwise leave them null.
- If user gives synonyms, normalize using this map: {synonyms}
- Keep JSON small; do not include analysis, only fields below.

Allowed:
{json.dumps(guide, ensure_ascii=False)}

Output schema (one of these):
{{"intent":"pivot","dim1":"...", "dim2":"...", "metric":"...", "year_from":1400, "year_to":1404, "top_n":10}}
OR
{{"intent":"rows","filters":{{"Ø³ØªÙˆÙ†":"Ù…Ù‚Ø¯Ø§Ø±"}}, "limit": 200}}
""".strip()

            try:
                plan_resp = client.chat.completions.create(
                    model="gpt-4o-mini",
                    messages=[{"role": "system", "content": system_prompt},
                              {"role": "user", "content": user_q}],
                    temperature=0,
                )
                plan_text = plan_resp.choices[0].message.content.strip()
                start = plan_text.find("{"); end = plan_text.rfind("}")
                if start == -1 or end == -1:
                    raise ValueError("No JSON plan returned.")
                plan = json.loads(plan_text[start:end+1])
            except Exception as e:
                msg = f"âš ï¸ Ø¨Ø±Ù†Ø§Ù…Ù‡â€ŒØ±ÛŒØ² Ù†ØªÙˆØ§Ù†Ø³Øª Ø¨Ø±Ù†Ø§Ù…Ù‡ Ø¨Ø¯Ù‡Ø¯: {e}"
                st.chat_message("assistant").write(msg)
                st.session_state.data_chat.append({"role": "assistant", "content": msg})
                plan = None

            answer = None
            if plan:
                try:
                    if plan.get("intent") == "pivot":
                        d1 = plan.get("dim1"); d2 = plan.get("dim2")
                        metric = plan.get("metric", "Ø§Ø±Ø²Ø´ Ø±ÛŒØ§Ù„ÛŒ")
                        y1 = plan.get("year_from"); y2 = plan.get("year_to")
                        top_n = int(plan.get("top_n") or 20)

                        if d1 not in allowed_dims or d2 not in allowed_dims:
                            raise ValueError("Invalid dimension(s).")
                        if metric not in allowed_metrics:
                            raise ValueError("Invalid metric.")

                        res = sb.rpc("pivot_2d_numeric", {
                            "dim1": d1, "dim2": d2, "metric": metric,
                            "year_from": int(y1) if y1 else None,
                            "year_to": int(y2) if y2 else None
                        }).execute()
                        df_ans = pd.DataFrame(res.data or [])
                        if not df_ans.empty:
                            df_ans = df_ans.sort_values("total_value", ascending=False).head(top_n)
                            st.dataframe(df_ans, use_container_width=True)
                            answer = f"Ù†ØªÛŒØ¬Ù‡â€ŒÛŒ Pivot Ø¨Ø±Ø§ÛŒ Â«{d1} Ã— {d2}Â» Ø±ÙˆÛŒ Â«{metric}Â»" + (f" Ø¯Ø± Ø¨Ø§Ø²Ù‡â€ŒÛŒ {y1}-{y2}" if y1 and y2 else "") + f" (Top {top_n})."
                        else:
                            answer = "Ù‡ÛŒÚ† Ù†ØªÛŒØ¬Ù‡â€ŒØ§ÛŒ Ø¨Ø±Ø§ÛŒ Ø§ÛŒÙ† Pivot Ù¾ÛŒØ¯Ø§ Ù†Ø´Ø¯."

                    elif plan.get("intent") == "rows":
                        filters = plan.get("filters") or {}
                        limit = int(plan.get("limit") or 200)
                        q = sb.table(TABLE).select("*")
                        for col, val in filters.items():
                            col = synonyms.get(col, col)
                            if col not in allowed_filter_cols:
                                continue
                            if isinstance(val, list):
                                q = q.in_(col, val)
                            else:
                                q = q.eq(col, val)
                        q = q.limit(limit)
                        res = q.execute()
                        df_ans = pd.DataFrame(res.data or [])
                        if not df_ans.empty:
                            st.dataframe(df_ans, use_container_width=True)
                            answer = f"{len(df_ans)} Ø±Ø¯ÛŒÙ Ù…Ø·Ø§Ø¨Ù‚ ÙÛŒÙ„ØªØ±Ù‡Ø§ Ù†Ù…Ø§ÛŒØ´ Ø¯Ø§Ø¯Ù‡ Ø´Ø¯ (Ø­Ø¯Ø§Ú©Ø«Ø± {limit})."
                        else:
                            answer = "Ø±Ø¯ÛŒÙÛŒ Ù…Ø·Ø§Ø¨Ù‚ Ø´Ø±Ø§ÛŒØ· Ù¾ÛŒØ¯Ø§ Ù†Ø´Ø¯."
                    else:
                        answer = "Ø¬Ù‡Øª Ù¾Ø§Ø³Ø® Ù†ÛŒØ§Ø² Ø§Ø³Øª Ù…Ø´Ø®Øµ Ú©Ù†ÛŒØ¯ Pivot Ù…ÛŒâ€ŒØ®ÙˆØ§Ù‡ÛŒØ¯ ÛŒØ§ Ø±Ø¯ÛŒÙâ€ŒÙ‡Ø§ÛŒ Ø®Ø§Ù…."

                except Exception as e:
                    answer = f"âš ï¸ Ø§Ø¬Ø±Ø§ÛŒ Ø¨Ø±Ù†Ø§Ù…Ù‡ Ø´Ú©Ø³Øª Ø®ÙˆØ±Ø¯: {e}"

            if answer is None:
                answer = "Ø³ÙˆØ§Ù„ Ø±Ø§ ÙˆØ§Ø¶Ø­â€ŒØªØ± Ø¨Ù¾Ø±Ø³ ÛŒØ§ Ù…Ø«Ø§Ù„ Ø¨Ø¯Ù‡ ØªØ§ Pivot ÛŒØ§ ÙÛŒÙ„ØªØ± Ù…Ù†Ø§Ø³Ø¨ Ø¨Ø³Ø§Ø²Ù…."

            st.chat_message("assistant").write(answer)
            st.session_state.data_chat.append({"role": "assistant", "content": answer})
